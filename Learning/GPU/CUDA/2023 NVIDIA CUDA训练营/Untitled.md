Day 1 学习要点总结：
1. __device__ 的返回类型可以不是void？

--可以的。但这种情况下，执行的效果可能需要其他方式返回了。（例如通过一个参数int *p, 然后写入p指向的空间），

2.一般返回值都通过指针参数返回比较好？

--都行，常见的习惯是标量值，可以直接作为返回值，每个线程超过1个值（例如需要返回10个float），则建议用指针。

 3. 和CPU上只能同时执行有限数量（常见的例如8核16个超线程的CPU）的十几个、几十个。GPU上能同时执行海量的线程数量，例如几十万、上百万。可以有效的发挥GPU设备的能力。而如何有效的管理这么大数量的线程，则需要"线程组织形式", 可以有效的管理、执行问题，避免混乱。

 4. 来自Fortran同学注意了，我们的GPU上的CUDA C语言，采用的是下标从0开始，而不是1.

5. 学校：1个grid /年级：1个block/班：1个block/你：1个线程

6. 指定超过ＳＭ中ｃｏｒｅ数量的话会怎样？无问题的。你可以理解成每个SM都是海量超线程的。例如我们本次例子的Jetson设备，1个SM只有128个SP，却可以同时执行2048个线程（你可以理解成16倍超线程）。再多也是可以的，用其他方式继续调度

7. 线程数目可以远大于物理core数目

 8. 1个block在一个sm里面执行，sm是什么？

--一般情况下，可以直接将GPU的SM理解成CPU的一个物理核心. 按SM划分有好多好处。例如一个GPU可以简单的通过横向扩充SM，即可扩大规模。例如1个block的线程限定给1个SM，可以让1个block的线程在SM内部高效的执行数据交换/交流/同步之类的。

9. 写cuda程序的时候能申请的最大线程数不是无限的, 最大的线程数量：1024*(2^31-1)*65535*65535

 10. 一个block有多少个线程是调用的时候自己指定的？而不是固定的？是自己（你）定的。

11. 如果两个进程运行，调用的函数都同时使用同一个blockid和threadid，会不会有冲突的？

--不会。依然各自是各自的线程（虽然两次启动线程的编号有重复的）。

 12. 不能直接将一次kernel启动理解成1个CPU上的process的。两回事。你理解成“一次能开辟很多线程的函数调用较好”。

13. 如果cuda申请的thread不足了，调用的函数会怎么样？？就是报错如何处理？

--如果你指定了超多的启动规模，超出了你卡的能力，会报告“无效启动配置”。

14. cudaDeviceSynchronize();是同步哪个步骤呢，是不同ｂｌｏｃｋ的计算结果么？

--CPU上的调用者等待GPU上的之前的所有进行中的异步任务完成。和GPU上的blocks之间互相同步（那个叫全局同步）无关。


Day 2 我们将学习：
1.     
CUDA编程模型---CUDA存储单元的使用与错误检测（2.1+2.2实验课）

  设备初始化

  GPU的存储单元

  GPU存储单元的分配与释放

  数据的传输

  数据与线程之间的对应关系

  CUDA应用程序运行时的错误检测

  CUDA中的事件

  利用事件进行计时

  实验课内容：编写MatrixMul程序，体验线程和数据的对应关系

  留课后作业

2.     
多种CUDA存储单元详解（2.3）

  CUDA中的存储单元种类

  CUDA中的各种存储单元的使用方法

  CUDA中的各种存储单元的适用条件

3.     
利用共享存储单元优化应用（2.4实验课）

  共享存储单元详解

  共享内存的Bank conflict

  利用共享存储单元进行矩阵转置和矩阵乘积

  实验课内容：编写Shared Memory优化过的矩阵乘法

介绍shared memory原理，介绍利用shared memory 优化的多种案例

  矩阵转置