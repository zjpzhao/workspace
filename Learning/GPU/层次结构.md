

## 线程和存储层次
>参考：[《CUDA C Programming Guide》(《CUDA C 编程指南》)导读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/53773183)
>和：[CUDA入门（深度学习向） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/495850310)

线程层次：thread→warp→block (shared mem)→Grid (global mem among blocks)
CTA=Block
### SM
SM是用来调度Block的流处理器簇，当线程块执行完之后会退出SM，并释放SM的资源（共享内存和计算资源等）以供其他Block的调度。多个线程块可以被分配到同一个SM上。在SM上同一个块内的多个线程进行线程级别并行。而同一线程内，指令利用指令级并行将单个线程处理成流水线。
32个**相邻的线程**会组成一个线程束(*Thread Warp*)，而一个线程束中的线程会运行同样的指令。因此一般线程块中线程的数量被安排为32的倍数。warp 是来自单个 CTA 的最大线程子集

### 不同level线程的访存权限
-   寄存器和本地内存绑定到了每个线程，其他线程无法访问。
-   同一个线程块内的线程，可以访问同一块共享内存。注意，即使两个线程块被调度到了同一个SM上，他们的共享内存也是隔离开的，不能互相访问。
-   Grid中的所有线程都可以自由读写全局内存。
-   常量内存和纹理内存只能被CPU端修改，GPU内的线程只能读取数据。


## PTX
“Parallel Thread Execution (PTX)” (“Parallel Thread Execution ISA”, p. 4)
并行线程执行 （PTX） 编程模型是显式并行的：PTX 程序指定并行线程数组的给定线程的执行。协作线程数组 （CTA） 是并发或并行执行内核的线程数组。

## Thread Hierarchy
执行内核的线程批次被组织为协作线程数组的网格，如下图所示。协作线程数组 （CTA） 实现 CUDA 线程块。
CTA 可以包含的线程数有上限。但是，执行相同内核的 CTA 可以一起批处理成一个 CTA 网格，这样在单个内核调用中可以启动的线程总数非常大。这是以减少线程通信和同步为代价的，因为不同 CTA 中的线程无法相互通信和同步。
主机向设备发出一系列内核调用。每个内核都作为一批线程执行，这些线程组织为一个 CTA 网格。也就是说 CTA 是一组执行同一内核程序的并发线程。Grid 是一组独立执行的 CTA。
![](https://docs.nvidia.com/cuda/parallel-thread-execution/graphics/thread-batching.png)

“The batch of threads that executes a kernel is organized as a grid of cooperative thread arrays as described in this section and illustrated in “Figure 1.” ([“Parallel Thread Execution ISA”, p. 6](zotero://select/library/items/2WPUCD43)) ([pdf](zotero://open-pdf/library/items/HAMZCXMT?page=20)) . Cooperative thread arrays (CTAs) implement CUDA thread blocks.” ([“Parallel Thread Execution ISA”, p. 4](zotero://select/library/items/2WPUCD43))

“A CTA encapsulates all synchronization and barrier primitives among a group of threads [1], [21]” (Nie et al., 2018, p. 751)

GPU通常配备大量内核，在NVIDIA术语中也称为流多处理器（SM）。每个内核都有其私有 L1 高速缓存、软件管理的暂存器存储器和一个大型寄存器文件。互连网络将所有内核连接到全局内存，全局内存由各种内存通道组成。每个内存通道都有一个共享的 L2 高速缓存，其关联的内存请求由 GDDR5 内存控制器处理。
- [x] block和CTA的关系？（一样的）

#### 设计CTA的好处
- 使GPU硬件能够放宽CTA的执行顺序，以实现最大的并行度
- 全局同步开销过大，所以让block 内的 warp 通过 barrier 同步来减小同步开销


warp是一个抽象架构，对程序员是透明的。
假设寄存器文件和缓存和内存等其他组件受 ECC 保护（几乎所有 GPU 都是这种情况）因此，我们在这里只考虑由于 ALU/LSU 中的瞬态单比特错误（也称为软错误）而导致的常见计算相关错误，即在不受 ECC 保护的组件中。这些故障可能导致错误的 ALU 输出，然后将其存储在目标寄存器中。

硬件角度
1. Grid：GPU (GPC) 级别的基本调度单位
2. Block (CTA)：SM 级别的基本调度单位
3. Warp：ALU 级别的基本调度单位

从资源分配和通信的角度来看
1. Grid：共享同样的 Kernel 和 Context
2. Block (CTA)：同一 CTA 的线程运行在同一个 SM，因此同一 CTA 中的不同线程可以通过 Shared Memory 通信，并共享 L1 cache（只读）。
3. Warp：32 宽度的 SIMD，占用同一块向量寄存器，线程间可以通过特殊指令交换寄存器中的数据。